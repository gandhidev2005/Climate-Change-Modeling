{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: installs (idempotent) and NLTK data\n",
        "import sys, subprocess\n",
        "\n",
        "def ensure(package):\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "\n",
        "for pkg in ['pandas', 'numpy', 'matplotlib', 'seaborn', 'nltk', 'scikit-learn']:\n",
        "    ensure(pkg)\n",
        "\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
        "except LookupError:\n",
        "    nltk.download('vader_lexicon')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and data loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_PATH = Path('climate_nasa.csv')\n",
        "\n",
        "# Read CSV\n",
        "raw_df = pd.read_csv(DATA_PATH)\n",
        "print(f\"Loaded {len(raw_df):,} rows, {raw_df.shape[1]} columns\")\n",
        "raw_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic cleaning and typing\n",
        "\n",
        "df = raw_df.copy()\n",
        "\n",
        "# Standardize column names\n",
        "expected_cols = ['date', 'likesCount', 'profileName', 'commentsCount', 'text']\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "# Parse datetime, coerce errors to NaT\n",
        "if 'date' in df.columns:\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "# Ensure numeric types\n",
        "for col in ['likesCount', 'commentsCount']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Text normalization\n",
        "if 'text' in df.columns:\n",
        "    df['text'] = df['text'].astype(str).fillna('').str.strip()\n",
        "    # Remove duplicate whitespace\n",
        "    df['text'] = df['text'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "# Drop rows with no text\n",
        "df = df[df['text'].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "print(df.dtypes)\n",
        "print(df.isna().sum())\n",
        "print(f\"Remaining rows: {len(df):,}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment analysis with VADER\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "sent_scores = df['text'].apply(sia.polarity_scores).apply(pd.Series)\n",
        "df = pd.concat([df, sent_scores], axis=1)\n",
        "\n",
        "# Label sentiment based on compound score\n",
        "sentiment_bins = pd.cut(\n",
        "    df['compound'], bins=[-1.0, -0.05, 0.05, 1.0], labels=['negative', 'neutral', 'positive'], include_lowest=True\n",
        ")\n",
        "df['sentiment'] = sentiment_bins.astype(str)\n",
        "\n",
        "df[['compound', 'neg', 'neu', 'pos', 'sentiment']].describe(include='all')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trend analysis over time\n",
        "\n",
        "if 'date' in df.columns and df['date'].notna().any():\n",
        "    tmp = df.set_index('date').copy()\n",
        "    monthly = tmp.resample('MS').agg({\n",
        "        'compound': 'mean',\n",
        "        'text': 'count',\n",
        "        'likesCount': 'mean',\n",
        "        'commentsCount': 'mean'\n",
        "    }).rename(columns={'text': 'num_comments'})\n",
        "\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
        "    monthly['compound'].plot(ax=axes[0], color='tab:green', title='Average monthly sentiment (compound)')\n",
        "    monthly['num_comments'].plot(ax=axes[1], color='tab:blue', title='Monthly comment volume')\n",
        "    (monthly[['likesCount', 'commentsCount']]).plot(ax=axes[2], title='Monthly avg engagement (likes, replies)')\n",
        "    plt.tight_layout()\n",
        "else:\n",
        "    print('Date column missing or unparsable; skipping time trends.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Engagement analysis: correlations and simple visuals\n",
        "\n",
        "# Text length as a proxy for effort/verbosity\n",
        "df['text_len'] = df['text'].str.len()\n",
        "\n",
        "eng_cols = ['likesCount', 'commentsCount']\n",
        "avail_eng = [c for c in eng_cols if c in df.columns]\n",
        "\n",
        "if avail_eng:\n",
        "    corr = df[avail_eng + ['compound', 'text_len']].corr(numeric_only=True)\n",
        "    print(corr)\n",
        "\n",
        "    sns.pairplot(df, vars=avail_eng + ['compound', 'text_len'], kind='reg', plot_kws={'scatter_kws': {'alpha': 0.2}})\n",
        "    plt.suptitle('Engagement vs sentiment and text length', y=1.02)\n",
        "    plt.show()\n",
        "else:\n",
        "    print('Engagement columns not present; skipping engagement analysis.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Topic modeling with LDA (scikit-learn)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "NUM_TOPICS = 5\n",
        "MAX_FEATURES = 5000\n",
        "\n",
        "# Vectorize\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english', max_features=MAX_FEATURES,\n",
        "                             token_pattern=r'(?u)\\b[a-zA-Z]{3,}\\b')\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=NUM_TOPICS, random_state=42, learning_method='batch')\n",
        "lda.fit(X)\n",
        "\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "def top_words_per_topic(model, feature_names, n_top_words=12):\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_idx = topic.argsort()[-n_top_words:][::-1]\n",
        "        topics.append((topic_idx, feature_names[top_idx]))\n",
        "    return topics\n",
        "\n",
        "topics = top_words_per_topic(lda, feature_names)\n",
        "for topic_idx, words in topics:\n",
        "    print(f\"Topic {topic_idx}:\", ', '.join(words))\n",
        "\n",
        "# Assign dominant topic to each document\n",
        "doc_topic = lda.transform(X)\n",
        "df['topic'] = doc_topic.argmax(axis=1)\n",
        "\n",
        "df[['topic', 'sentiment']].groupby(['topic', 'sentiment']).size().unstack(fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save enriched dataset and some figures\n",
        "out_dir = Path('analysis_outputs')\n",
        "out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "df_out_path = out_dir / 'climate_comments_enriched.csv'\n",
        "df.to_csv(df_out_path, index=False)\n",
        "print(f'Saved enriched CSV to {df_out_path.resolve()}')\n",
        "\n",
        "# Bar plot: sentiment distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "df['sentiment'].value_counts().reindex(['negative','neutral','positive']).plot(kind='bar', color=['#d62728','#7f7f7f','#2ca02c'])\n",
        "plt.title('Sentiment distribution')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "fig1 = out_dir / 'sentiment_distribution.png'\n",
        "plt.savefig(fig1, dpi=150)\n",
        "print(f'Saved {fig1.resolve()}')\n",
        "plt.show()\n",
        "\n",
        "# Bar plot: topics top words\n",
        "topic_top_words = {topic: words for topic, words in topics}\n",
        "fig, axes = plt.subplots(len(topic_top_words), 1, figsize=(10, 2*len(topic_top_words)))\n",
        "if len(topic_top_words) == 1:\n",
        "    axes = [axes]\n",
        "for t, ax in enumerate(axes):\n",
        "    words = topic_top_words[t]\n",
        "    ax.bar(range(len(words)), [1]*len(words))\n",
        "    ax.set_xticks(range(len(words)))\n",
        "    ax.set_xticklabels(words, rotation=45, ha='right')\n",
        "    ax.set_title(f'Topic {t} top words')\n",
        "plt.tight_layout()\n",
        "fig2 = out_dir / 'topics_top_words.png'\n",
        "plt.savefig(fig2, dpi=150)\n",
        "print(f'Saved {fig2.resolve()}')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
